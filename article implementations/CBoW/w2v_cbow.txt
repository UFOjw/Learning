import gensim.downloader as api
import torch
import torch.nn as nn
import torch.optim as optim

from typing import List, Optional
from tqdm import tqdm
import sys


def one_hot_tensor(idx, n):
    tensor = torch.zeros(n)
    tensor[idx] = 1
    return tensor


class Word2Vec(nn.Module):
  def __init__(self, text: List[list], embedding_size: int = 100, lr: float = 0.05, window_size: int = 5, device: torch.device = 'cuda'):
    super(Word2Vec, self).__init__()
    self.device = device
    self.dictionary = self.get_dict(text)
    self.dict_size = len(self.dictionary)
    self.embedding_size = embedding_size
    self.embeddings = nn.Embedding(self.dict_size, embedding_size).to(self.device)
    self.linear = nn.Linear(embedding_size, self.dict_size).to(self.device)
    self.lr = lr
    self.window_size = window_size

  def get_dict(self, text: List[list]) -> dict:
    """
    Function to extract unique words from a text along with their indexes.

    Args:
    text: list[list]
        Input text containing words.

    Returns:
    dict:
        A dictionary where keys are unique words and values are lists of indexes where the word appears.
    """
    words = []
    for sentence in text:
      for word in sentence:
        words.append(word)
    self.cnt_words = len(words)
    return dict(zip(sorted(set(words)), range(len(set(words)))))

  def word2idx(self, word: str) -> Optional[int]:
    """Get index by specifying a word"""
    if word in self.dictionary:
      return self.dictionary[word]
    else:
      return None

  def forward(self, context_words: List[int]) -> torch.Tensor:
    """
    Forward pass of the Word2Vec model.

    Args:
    context_words: list[int]
        List of indexes of context words.

    Returns:
    torch.Tensor:
        Predicted target word embedding.
    """
    context_vectors = self.embeddings(torch.tensor(context_words).to(self.device))
    predicted_vector = torch.mean(context_vectors, dim=0)
    output = self.linear(predicted_vector)
    return output

  def fit(self, corpus: List[list]):
    optimizer = optim.AdamW(self.parameters(), lr=self.lr)
    loss = nn.CrossEntropyLoss()

    total_loss = 0
    for num_s, sentence in enumerate(tqdm(corpus, desc='Corpus')):
      words_idx = [self.dictionary[word] for word in sentence]

      for i, center_word in enumerate(tqdm(words_idx, desc='Percent of words in sentence', leave=False, file=sys.stdout)):
        context_words = []
        for j in range(i - self.window_size, i + self.window_size):
          if j != i and 0 <= j < len(sentence):
            context_words.append(words_idx[j])
        predicted = self.forward(context_words)
        target_emb = one_hot_tensor(center_word, self.dict_size).to(self.device)

        optimizer.zero_grad()
        output = loss(predicted, target_emb)
        total_loss += output
        output.backward()
        optimizer.step()

      if num_s % 20 == 0:
        print(f'Sentence {num_s}, Loss: {total_loss / self.cnt_words}')


  def predict(self, context_words: List[str]) -> torch.Tensor:
    """
    Predict target word embedding based on context words.

    Args:
    context_words: list[int]
        List of indexes of context words.

    Returns:
    torch.Tensor:
        Predicted target word embedding.
    """
    with torch.no_grad():
      words_idx = [self.dictionary[word] for word in context_words]
      output = self.forward(words_idx)

    return output


corpus = api.load('text8')
dataset = []
for sentence in corpus:
    dataset.append(sentence)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Word2Vec(text=dataset, device=device)

model.fit(dataset)

torch.save(model.state_dict(), 'word2vec_model.pth')
